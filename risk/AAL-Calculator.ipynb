{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0,
     "end_time": "2018-11-05T19:39:26.725159",
     "exception": false,
     "start_time": "2018-11-05T19:39:26.725159",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# __AAL Calculator__ \n",
    "\n",
    "PYTHON 3.6\n",
    "  \n",
    "  \n",
    "Overview: This notebook was created to document the development of the Atkins FEMA AAL loss spreadsheet into python\n",
    "\n",
    "Updated: 2019-10-07\n",
    "\n",
    "by Stephen Duncan: sduncan@dewberry.com <br/>\n",
    "edited by Seth Lawler: slawler@dewberry.com\n",
    "\n",
    "refactored by Alec Brazeau: abrazeau@dewberry.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 5.552513,
     "end_time": "2018-11-05T19:39:32.444202",
     "exception": false,
     "start_time": "2018-11-05T19:39:26.891689",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import pathlib as pl\n",
    "import pandas as pd\n",
    "from time import time\n",
    "from scipy.interpolate import interp1d\n",
    "from multiprocessing import Pool, cpu_count\n",
    "\n",
    "root = pl.Path(root_dir)\n",
    "sys.path.append(str(root.parent / 'core'))\n",
    "from risk_refactor import *\n",
    "\n",
    "pd.options.display.max_columns = 325\n",
    "pd.options.display.max_rows = 100\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check if it is pluvial or fluvial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project = scen.split('_')[0]\n",
    "model_name = scen.split('_')[1]\n",
    "book = scen.split('_')[2]\n",
    "if model_name[0] == 'P':\n",
    "    pluvial = True\n",
    "else:\n",
    "    pluvial = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "# Create Area Depth-Damage Curves\n",
    "\n",
    "- Based on the Damage Categories provided, damage curves can be aggregated and averaged to develop loss data specific to the study area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "defaultHazusDDFn_path = root / 'hazusdepthdmgfns' / 'Building_DDF_Full_LUT_Hazus3p0.json'\n",
    "df_BDDFn = pd.read_json(str(defaultHazusDDFn_path), orient = 'index')\n",
    "df_BDDFn = hazusID_to_depth(df_BDDFn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_agg = aggregate_ddf_curves(df_BDDFn, curve_groups, plot=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alter the HAZUS DDf curves to comply with actuaries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set curve for single family 1 story no basement\n",
    "df_agg.loc[-1, 'singFam_1Story_NoBasement']= 0.6\n",
    "df_agg.loc[-0.000000000001, 'singFam_1Story_NoBasement']= 0.6\n",
    "\n",
    "# set curve for single family 2 story no basement\n",
    "df_agg.loc[-1, 'singFam_2Story_NoBasement']= 0.6\n",
    "df_agg.loc[-0.000000000001, 'singFam_2Story_NoBasement']= 0.6\n",
    "\n",
    "# set curve for single family 3 story no basement\n",
    "df_agg.loc[-1, 'singFam_3Story_NoBasement']= 0.0\n",
    "df_agg.loc[-0.000000000001, 'singFam_3Story_NoBasement']= 0.0\n",
    "\n",
    "# set curve for mobile home\n",
    "df_agg.loc[-0.000000000001, 'mobileHome']= 0.75\n",
    "\n",
    "df_agg = df_agg.sort_index()\n",
    "display(df_agg.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0,
     "end_time": "2018-11-05T19:39:34.643270",
     "exception": false,
     "start_time": "2018-11-05T19:39:34.643270",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Weights for modelled runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if pluvial:\n",
    "    try:\n",
    "        df_weights = pd.read_csv(weights_path, index_col='event_id')\n",
    "        df_weights.rename(columns={'weight':'RunWeight'}, inplace=True)\n",
    "    except ValueError as e:\n",
    "        print(e)\n",
    "        df_weights = pd.read_csv(weights_path)\n",
    "        df_weights.rename(columns={'Weight':'RunWeight', 'Run':'event_id'}, inplace=True)\n",
    "        df_weights = df_weights.set_index('event_id')\n",
    "    weights_dict = dict(zip(df_weights.index, df_weights.RunWeight))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prep WSE/Attribute Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if pluvial:\n",
    "    loss_functs = {hz_cat: interp1d(df_agg.index, df_agg[hz_cat]) for hz_cat in df_agg.columns}\n",
    "    lowest_ddf_elev = df_agg.index.min()\n",
    "    highest_ddf_elev = df_agg.index.max()\n",
    "    df_wse = pd.read_csv(wse_file, index_col=structure_cols['Unique Building ID'])\n",
    "    col_events = [c for c in df_wse.columns if '_E' in c]\n",
    "    dict_cols = [structure_cols['Building Deduction'], structure_cols['Ground Elevation'],\n",
    "                 structure_cols['First Floor Height'], structure_cols['Building Limit'],\n",
    "                 structure_cols['Damage Code']] + col_events\n",
    "    wse_dict = df_wse[dict_cols].T.to_dict()\n",
    "    unique_ids = df_wse.index.tolist()\n",
    "    allargs = [(uid, wse_dict, weights_dict,\n",
    "                col_events, loss_functs,\n",
    "                lowest_ddf_elev, highest_ddf_elev,\n",
    "                structure_cols) \n",
    "               for uid in unique_ids]\n",
    "    \n",
    "else:\n",
    "    wse_file = 's3://pfra/RiskAssessment/{0}/Results/{1}/WSE_{0}_{1}_{2}.csv'.format(project, model_name, book)\n",
    "    breach_prob_file = 's3://pfra/RiskAssessment/{0}/BreachAnalysis/{0}_{1}_raw_prob_table.csv'.format(project, model_name)\n",
    "    weights_file = 's3://pfra/RiskAssessment/{0}/BreachAnalysis/{0}_{1}.xlsx'.format(project, model_name)\n",
    "    dfwse = pd.read_csv(wse_file, index_col='plus_code')\n",
    "    dfw = pd.read_excel(weights_file, sheet_name='Event_Weights', index_col=0 )[['Overall Weight']]\n",
    "    dfbp = pd.read_csv(breach_prob_file, sep='\\t', index_col=0)\n",
    "    for col in dfbp.columns:\n",
    "        dfbp.rename(columns={col:col.split('_')[1]}, inplace=True)\n",
    "    events_data_object = FluvialEvents(dfw, dfbp)\n",
    "    wse_results = [c for c in dfwse.columns if '_E' in c]\n",
    "    thin_df = dfwse[wse_results].copy()\n",
    "    thin_df['Max'] = thin_df.max(axis=1)\n",
    "    non_zeros = thin_df[thin_df['Max'] > 0].index\n",
    "    print('Non-Zeros Points', len(non_zeros))\n",
    "    zero_pts = [x for x in thin_df.index if x not in non_zeros]\n",
    "    print('Zeros Points', len(zero_pts))\n",
    "    zero_pts_results = [(pcode, 0) for pcode in zero_pts]\n",
    "    compute_matrix = dfwse.loc[non_zeros].drop(columns='geom').copy()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if pluvial:\n",
    "    st = time()\n",
    "    with Pool(cpu_count()) as p:\n",
    "        results = p.map(calc_pluv_aal_mp, allargs)\n",
    "    print(round((time()-st)/60, 2), 'minutes to process')\n",
    "        \n",
    "else:\n",
    "    step = 1000\n",
    "    results = zero_pts_results\n",
    "    walltime = 0\n",
    "    npoints =  compute_matrix.shape[0]\n",
    "\n",
    "    with Pool(cpu_count()) as p:\n",
    "        for i in range(0, compute_matrix.shape[0], step):\n",
    "            pcodes = compute_matrix.index.tolist()[i:i+step]\n",
    "            wse_slice = compute_matrix.loc[pcodes].copy()\n",
    "            allargs = [(p_code, events_data_object, df_agg, wse_slice.loc[p_code], structure_cols) for p_code in wse_slice.index]\n",
    "\n",
    "            st = time()\n",
    "            \n",
    "            slice_results = p.map(calc_fluv_aal_mp_functions, allargs)\n",
    "            results += slice_results\n",
    "            wtime = time()-st\n",
    "            walltime += wtime\n",
    "            print('Progress = {}%'.format(round(100*(i/npoints)),2), '\\tBatchtime =', round(wtime,2), 'seconds\\n')\n",
    "\n",
    "        print(step, round(walltime/60, 2), 'minutes to process')\n",
    "\n",
    "aal_results = {k:v for k,v in results}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "# Save Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Output AAL to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_AAL_only = pd.DataFrame.from_dict([aal_results]).T.rename(columns={0:'AAL'})\n",
    "df_AAL_only.index.name = structure_cols['Unique Building ID']\n",
    "df_AAL_only.to_csv(out_AAL)\n",
    "print('AAL Results saved:\\n\\n{}'.format(out_AAL))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anom_threshold = 3000\n",
    "anoms = df_AAL_only[df_AAL_only['AAL'] > anom_threshold].copy()\n",
    "anoms.to_csv(out_AAL.replace('.csv', '_anoms.csv'))\n",
    "anoms.AAL.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Display Loss Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown\n",
    "tot = df_AAL_only.AAL.sum()\n",
    "mx = df_AAL_only.AAL.max()\n",
    "avg = df_AAL_only.AAL.mean()\n",
    "display(Markdown('## Sum: ${0:,.2f}'.format(tot)))\n",
    "display(Markdown('## Max: ${0:,.2f}'.format(mx)))\n",
    "display(Markdown('## Mean: ${0:,.2f}'.format(avg)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "# END "
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "nteract": {
   "version": "0.15.0"
  },
  "papermill": {
   "duration": 12.760471,
   "end_time": "2018-11-05T19:39:36.691309",
   "environment_variables": {},
   "exception": true,
   "output_path": "T:\\CCSI\\TECH\\FEMA\\2018_SO4_Innovation\\AAL_Comps\\East_Jackson\\AnalysisPart2\\Rawresults.ipynb",
   "parameters": {
    "TableauTbls": "T:\\CCSI\\TECH\\FEMA\\2018_SO4_Innovation\\AAL_Comps\\East_Jackson\\AnalysisPart2\\ANALYSIS\\AllQ_AllD_Results\\TableauTbls_Rawresults.pkl",
    "loss_usd_allruns_out": "T:\\CCSI\\TECH\\FEMA\\2018_SO4_Innovation\\AAL_Comps\\East_Jackson\\AnalysisPart2\\ANALYSIS\\AllQ_AllD_Results\\loss_usd_allruns_Rawresults.pkl",
    "loss_usd_out": "T:\\CCSI\\TECH\\FEMA\\2018_SO4_Innovation\\AAL_Comps\\East_Jackson\\AnalysisPart2\\ANALYSIS\\AllQ_AllD_Results\\loss_usd_Rawresults.pkl",
    "max_wse_file": "T:\\CCSI\\TECH\\FEMA\\2018_SO4_Innovation\\AAL_Comps\\East_Jackson\\AnalysisPart2\\AllCases\\RawResults.csv",
    "structure_id_field": "accntnum",
    "structures_path": "T:\\CCSI\\TECH\\FEMA\\2018_SO4_Innovation\\AAL_Comps\\East_Jackson\\AnalysisPart2\\Structures\\EastJacksonMS_SP_uniform.shp",
    "weights_path": "T:\\CCSI\\TECH\\FEMA\\2018_SO4_Innovation\\AAL_Comps\\East_Jackson\\AnalysisPart2\\east_jackson_weights.csv",
    "wse_root": "T:\\CCSI\\TECH\\FEMA\\2018_SO4_Innovation\\AAL_Comps\\East_Jackson\\AnalysisPart2\\AllCases"
   },
   "start_time": "2018-11-05T19:39:23.930838",
   "version": "0.15.1"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
