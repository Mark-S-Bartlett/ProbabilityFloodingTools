{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prep AAL Inputs\n",
    "## Create WSE csv files for the AAL tool\n",
    "\n",
    "Workflow \n",
    "\n",
    "1. Read and join WSE csv output files from s3.  \n",
    "2. Attribute ground elevations given input point file. \n",
    "3. Join attributes to the building attribute files stored on s3 for each 'book'\n",
    "\n",
    " - Uniform\n",
    " - Uncorrelated\n",
    " - Market Basket \n",
    "     \n",
    "     \n",
    "4. Write input for AAL Tool\n",
    "5. Create Weights file for Pluvial*\n",
    "\n",
    "**Fluvial Weights created using MC_BreachSimulator*\n",
    "\n",
    "Author: Alec Brazeau - abrazeau@dewberry.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gdal\n",
    "gdal.UseExceptions()\n",
    "import os\n",
    "import sys; sys.path.append('../core')\n",
    "import pandas as pd\n",
    "from zipfile import ZipFile\n",
    "from s3utils import *\n",
    "from prep import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Parameters\n",
    "# TRI = \"\"\n",
    "# wse_dir = \"C:\\\\Users\\\\Administrator\\\\Desktop\\\\probmod-tools\\\\risk\\\\outputs\\\\MeridianHills\\\\F01\\\\WSE\"\n",
    "# weights_dir = \"C:\\\\Users\\\\Administrator\\\\Desktop\\\\probmod-tools\\\\risk\\\\outputs\\\\MeridianHills\\\\F01\\\\Weights\"\n",
    "# project = 'MeridianHills'\n",
    "# model = 'F01'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'F' in model:\n",
    "    model_type = 'Fluvial'\n",
    "else:\n",
    "    model_type = 'Pluvial'\n",
    "    \n",
    "s3point_zip = r's3://pfra/RiskAssessment/{0}/Points/{0}_{1}.zip'.format(project, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Weights File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if model_type == 'Pluvial':\n",
    "    for i in range(99):\n",
    "        try:\n",
    "            s3WeightsPath = 's3://pfra/{0}/Data/Forcing/{1}/Outputs/{0}_{2}_D{3}_Weights{4}.json'.format(project, model_type, model, str(i).zfill(2), TRI)\n",
    "            print(s3WeightsPath)\n",
    "            weights = make_weights_df(s3WeightsPath)\n",
    "            weights.to_csv(os.path.join(weights_dir, '_'.join([project, model, 'weights{}.csv'.format(TRI)])))\n",
    "            print('Weights written to {}'.format(wse_dir))\n",
    "            break\n",
    "        except AttributeError as e:\n",
    "            print('\\nError on {}\\n'.format(str(i).zfill(2)))\n",
    "else:\n",
    "    weights_file = 's3://pfra/RiskAssessment/{0}/BreachAnalysis/{0}_{1}.xlsx'.format(project, model)\n",
    "    weights = pd.read_excel(weights_file, sheet_name='Event_Weights', index_col=0 )[['Overall Weight']]\n",
    "#     weights.to_csv(os.path.join(weights_dir, '_'.join([project, model, 'weights{}.csv'.format(TRI)])))\n",
    "#     print('Weights written to {}'.format(wse_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the WSE dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search s3 for processed wse's for project/model\n",
    "s3files = s3List('pfra', '{}/{}'.format(project, model), '', '.csv')\n",
    "print(len(s3files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3files = [f for f in s3files if not 'BreachLocation' in f]\n",
    "print(len(s3files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check to make sure there is a WSE csv for each event\n",
    "if model_type == 'Pluvial':\n",
    "    wt_events = weights.event_id.tolist()\n",
    "    csv_events = [os.path.basename(x).split('{}_{}_'.format(project, model))[-1].replace('.csv', '') for x in s3files]\n",
    "else:\n",
    "    wt_events = weights.index.tolist()\n",
    "    csv_events = [os.path.basename(x).split('{}_{}_'.format(project, model))[-1].replace('.csv', '').split('_')[1] for x in s3files]\n",
    "\n",
    "for e in wt_events:\n",
    "    assert e in csv_events, 'WSE CSV file for event {} is missing!'.format(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the missing runs\n",
    "missing = []\n",
    "for e in wt_events:\n",
    "    if not e in csv_events:\n",
    "        missing.append(e)\n",
    "print(len(missing), 'Missing events:', missing)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Only gather WSEs for events we have weights for\n",
    "if model_type == 'Pluvial':\n",
    "    reset_weights = weights.set_index('event_id')[['weight']].copy()\n",
    "\n",
    "files_to_use = []\n",
    "for e in wt_events:\n",
    "    for f in s3files:\n",
    "        if e in f:\n",
    "            if model_type == 'Pluvial':\n",
    "                if reset_weights.loc[e].values[0] != 0:\n",
    "                    files_to_use.append(f)\n",
    "            else:\n",
    "                if weights.loc[e].values[0] != 0:\n",
    "                    files_to_use.append(f)\n",
    "print('Using {} files.'.format(len(files_to_use)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read in CSV's for all points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wse_only = make_WSE_df(s3files)\n",
    "wse_only.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pull elevations at each point\n",
    "### Download and extract points shapefile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3download([s3point_zip], wse_dir)\n",
    "localzip = os.path.join(wse_dir, os.path.basename(s3point_zip))\n",
    "with ZipFile(localzip, 'r') as zip_ref:\n",
    "    zip_ref.extractall(localzip.replace('.zip', ''))\n",
    "points_shapefile = glob(os.path.join(localzip.replace('.zip', ''), '*shp'))[0]\n",
    "print('Points Shapefile downloaded and extracted to: {}'.format(points_shapefile))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve the elevations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elev = get_elevations(project, model, points_shapefile)\n",
    "elev.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Join Input Data\n",
    " - WSE results from events\n",
    " - Ground elevation from topo\n",
    " - Actuarial data fields (buildings attributes)\n",
    " \n",
    "### Save Final Inputs for AAL Tool to output directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attribute_files = s3List('pfra', 'RiskAssessment/{}/Attributes/{}'.format(project, model), '', '.csv')\n",
    "\n",
    "for f in attribute_files:\n",
    "    if 'Weights' in f or 'AAL' in f:\n",
    "        continue\n",
    "    attrs = load_s3_csv(f).set_index('plus_code')\n",
    "    wse = attrs.merge(wse_only, left_index=True, right_index=True)\n",
    "    final_wses = pd.concat([wse, elev], join='inner', axis=1)\n",
    "    if TRI:\n",
    "        final_wses.to_csv(os.path.join(wse_dir, 'WSE_' + os.path.basename(f).replace('.', TRI + '.')))\n",
    "        print('{} Complete'.format(os.path.basename(f).replace('.', TRI + '.')))\n",
    "    else:\n",
    "        final_wses.to_csv(os.path.join(wse_dir, 'WSE_' + os.path.basename(f)))\n",
    "        print('{} Complete'.format(os.path.basename(f)))\n",
    "          \n",
    "print(\"\\nOutputs Saved to {}\".format(wse_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# END"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "nteract": {
   "version": "0.15.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
