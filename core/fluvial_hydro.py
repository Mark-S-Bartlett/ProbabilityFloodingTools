# Brief description...
#
# Author: Shane Putnam (sputnam@Dewberry.com)
#
# Copyright: Dewberry
#
# --------------------------------------------------------------------------#
import time
import shutil
import logging
import warnings
import datetime
import geopandas as gpd
from hydrographs import*
import scipy.stats as stats
from scipy.stats import norm
from scipy.integrate import trapz
from scipy.optimize import minimize
from scipy.interpolate import interp1d
from IPython.core.display import display
import matplotlib as mpl
import plotly.graph_objs as go
from plotly.offline import iplot
from matplotlib import pyplot as plt
from matplotlib.ticker import FuncFormatter
logging.basicConfig(level=logging.ERROR)


plib = "pathlib.Path"


def peakfq_inp(q_df: pd.DataFrame, bcn_num: int, inp_file: plib) -> None:
    """Formats the peak streamflow data contained within the passed dataframe
       and saves it to a .inp file, which is read by PeakFQ.
    """
    cur_yr = datetime.datetime.today().year
    str1 = '3{0}       '.format(str(bcn_num).zfill(8))
    if type(q_df['Q_cfs'][0]) == str:
        q_df['Q_cfs'] = q_df['Q_cfs'].apply(lambda x: x.replace(',', ''))
    with open(inp_file, 'w') as f:
        for i in q_df.index:
            date = q_df.loc[i]['Date']
            yr_len = len(date.split('/')[2])
            if yr_len == 2:
                strp_date = datetime.datetime.strptime(date, '%m/%d/%y')
            if yr_len == 4:
                strp_date = datetime.datetime.strptime(date, '%m/%d/%Y')
            if cur_yr <= strp_date.year:
                strp_date = strp_date.replace(year=strp_date.year - 100)
            htime_string = datetime.datetime.strftime(strp_date, '%Y%m%d')
            q = str(q_df.loc[i]['Q_cfs'])
            str2 = htime_string + ' ' + q + '.'
            string = str1 + str2 + '\n'
            f.write(string)
        return None


def peakfq_psf(inp_file: plib, prt_file: plib, psf_file: plib, bcn_num: int,
               reg_skew: float, reg_skew_std: float) -> None:
    """Builds a specification file for running PeakFQ. This file includes the
       paths to the input and output files as well as analysis
       parameters/options, including the regional skew and the regional skew
       standard deviation.
    """
    str_base = "I ASCI {0}\nO File {1}".format(inp_file, prt_file)
    str_sites = ('\nStation {0}\n     GenSkew {1}\n     SkewSE '
                 '{2}'.format(str(bcn_num).zfill(8), reg_skew, reg_skew_std))
    string = str_base + str_sites
    with open(psf_file, 'w') as f:
        f.write(string)
    return None


def read_peakfq_prt(prt_file: plib, outputs_dir: plib, base_filename: str,
                    bcn_num: int, verbose: bool = True) -> None:
    """Extracts the flow frequency curve from the output file (.prt)
       generated by PeakFQ and saves it to a dataframe.
    """
    line_breaks = [(1, 9), (10, 17), (44, 55), (59, 68)]
    with open(prt_file) as file:
        lines = file.readlines()
        for i, line in enumerate(lines):
            if 'TABLE 4' in line:
                skiprows = i + 6
            elif '*Note' in line:
                endrow = i - 1
    num_rows = endrow - skiprows
    df = pd.read_fwf(prt_file, skiprows=skiprows, colspecs=line_breaks,
                     names=['AEP', 'Q_cfs', '5_CL', '95_CL'])
    df = df[0:num_rows]
    for col in df.columns:
        df[col] = pd.to_numeric(df[col])
    df['RI'] = df['AEP'].apply(lambda x: np.around((1 / x), 4))
    df = df.set_index('AEP')
    df = df[['RI', 'Q_cfs', '5_CL', '95_CL']]
    df.to_csv(outputs_dir / '{0}_Q{1}.csv'.format(base_filename, bcn_num))
    if verbose:
        print(display(df.head()))
    return


def peakq_dfs_to_dic(inputs_dir: plib, keyword: str,
                     verbose: bool = True) -> dict:
    """Load the peak streamflow data from each *.csv file into a dataframe
       and store all of the dataframes in a dictionary. The peak streamflow
       data should be downloaded from the USGS and saved with 'Date' and
       'Q_cfs' columns. Each *.csv file should have the gage ID and the
       passed keyword which is used to uniquely identify the peak streamflow
       files within the passed folder.
    """
    input_dic = {}
    for f in inputs_dir.glob('*.csv'):
        if keyword in f.stem:
            df = pd.read_csv(f)
            if type(df['Q_cfs'][0]) == str:
                df['Q_cfs'] = df['Q_cfs'].apply(lambda x: int(x.replace(',',
                                                                        '')))
            df = df.sort_values(by=['Q_cfs']).reset_index(drop=True)
            gage = f.stem.split('_')[0]
            input_dic[gage] = df
    if verbose:
        print('{} input files found'.format(len(input_dic.keys())))
    return input_dic


def ffc_dfs_to_dic(outputs_dir: plib, keyword: str,
                   verbose: bool = True) -> dict:
    """Load the flow frequency curves from each *.csv file created by the
       PeakFQ_FlowFrequency.ipynb into a dataframe and store all of the
       dataframes in a dictionary.
    """
    output_dic = {}
    for f in outputs_dir.glob('*.csv'):
        if keyword in f.stem:
            df = pd.read_csv(f, index_col='AEP')
            gage = f.stem.split('_')[3].replace('Q', '')
            output_dic[gage] = df
    if verbose:
        print('{} output files found'.format(len(output_dic.keys())))
    return output_dic


def point_slope_eq(m: float, x: np.ndarray, x1: float,
                   y1: float) -> np.ndarray:
    """Calculates the y-value (dependent variable) of the point-slope equation
       of a line for each x-value (independent variable) within the passed
       array of x-values.
    """
    y = m * (x - x1) + y1
    return y


def sum_square_resid(y: np.ndarray, ypred: np.ndarray) -> float:
    """Calculates the sum of the squared residual for the passed observed and
       predicted arrays.
    """
    ssr = sum((y - ypred) ** 2)
    return ssr


def total_sum_squares(y: np.ndarray) -> float:
    """Calculates the total sum of squares for the passed array.
    """
    tss = sum((y - y.mean()) ** 2)
    return tss


def coeff_of_deter(y: np.ndarray, ypred: np.ndarray,
                   num_dig: int = 4) -> float:
    """Calculates the coefficient of determination (R^2) for the passed
       observed and predicted arrays.
    """
    ssr = sum_square_resid(y, ypred)
    tss = total_sum_squares(y)
    r2 = np.round(1-(ssr/tss), num_dig)
    return r2


def calc_ypred_ssr(m: float, x: np.ndarray, y: np.ndarray, x1: float,
                   y1: float) -> float:
    """Calculates the y-value (dependent variable) of the point-slope equation
       given x, x1, y1, and m, and calculates the sum of the squared residual
       between the calculated y-value (predicted) vs. the passed y-value
       (observed).
    """
    ypred = point_slope_eq(m, x, x1, y1)
    ssr = sum_square_resid(y, ypred)
    return ssr


def calc_ffc_slope(output_dic: dict, x1: float = 0.5, num_dig: int = 4,
                   verbose: bool = True) -> dict:
    """Calculates the slope of each flow frequency curve within the passed
       dictionary using the point slope equation and the specified x1 value.
    """
    gagestats = {}
    for k, v in output_dic.items():
        y1 = v.loc[x1]['Q_cfs']
        x = np.array([x for x in v.index if x >= x1])
        y = np.array(v.loc[x]['Q_cfs'])
        m0 = np.array(stats.linregress(x, y)[0])
        solution = minimize(calc_ypred_ssr, m0, args=(x, y, x1, y1))
        m = np.round(solution.x[0], num_dig)
        ypre = point_slope_eq(m, x, x1, y1)
        r2 = coeff_of_deter(y, ypre, num_dig)
        gagestats[k] = {'x1': x1, 'y1': y1, 'x': x, 'y': y, 'ypre': ypre,
                        'm': m, 'r2': r2}
        if verbose:
            print('Gage ID: {0}, slope: {1}, R^2: {2}'.format(k, int(m), r2))
    return gagestats


def percent_diff(v1: float, v2: float) -> float:
    """Calculates the percent difference between two values when both values
       have the same sign (negative or positive).
    """
    assert v1 and v2 > 0 or v1 and v2 < 0, 'Both v1 and v2 need the same sign'
    v1 = abs(v1)
    v2 = abs(v2)
    pdif = np.round(abs(v1 - v2) / ((v1 + v2) / 2) * 100.0, 2)
    return pdif


def percent_error(approx: float, exact: float) -> float:
    """Calculates the percent error between an approximate value and the
       exact value.
    """
    pe = np.round(abs(approx-exact)/abs(exact)*100.0, 2)
    return pe


class SynthGage:
    """Calculate synthetic gage statistics, including the station skew,
       weighted skew, and the log-Person Type III flow frequency
       distribution.
    """
    def __init__(self, discharge: list):
        self._records = discharge
        self._n = len(discharge)
        self._log = np.log10(discharge)
        self._mean = np.mean(self._log)
        self._std = np.std(self._log, ddof=1)

    def station_skew(self) -> float:
        """The station (local) skew coefficient.
        """
        return (self._n / ((self._std ** 3) * (self._n - 1) * (self._n - 2)))\
            * np.sum((self._log - self._mean) ** 3)

    def station_mse(self) -> float:
        """The mean square errors(MSE) of the station (local) skew.
        """
        abs_sskew = np.abs(self.station_skew())
        a = -0.33 + 0.08 * abs_sskew
        b = 0.94 - 0.26 * abs_sskew
        if abs_sskew > 0.90:
            a = -0.52 + 0.30 * abs_sskew
        if abs_sskew > 1.50:
            b = 0.55
        return 10 ** (a - b * np.log10(self._n / 10.0))

    def weighted_skew(self, rskew: float, rskew_mse: float) -> float:
        """The weighted skew coefficient. This skew is the average of the
           regional and station skews weighted by their respective mean square
           errors.
        """
        sskew = self.station_skew()
        sskew_mse = self.station_mse()
        w = rskew_mse / (sskew_mse + rskew_mse)
        return w * sskew + (1 - w) * rskew

    def lp3_values(self, aep: list, rskew: float, rskew_mse: float) -> list:
        """Calculates the discharge for the passed annual exceedance
           probabilities using the log-Person Type III distribution.
        """
        skew = self.weighted_skew(rskew, rskew_mse)
        mu = self._mean
        std = self._std
        q = []
        for ep in aep:
            pct = 1 - ep
            lp3_val = int(10 ** stats.pearson3.ppf(pct, skew=skew, loc=mu,
                                                   scale=std))
            q.append(lp3_val)
        return q


def id_event_directories(s3_path: str, input_dir: plib, events_filename: str,
                         verbose: bool = True) -> list:
    """Given a parent directory on AWS S3, this function returns a list of
       the subdirectories. For PFRA and for a parent directory corresponding
       to a particular pluvial model, the list of subdirectories correspond
       to a list of the modeled events.
    """
    event_txt = input_dir / '{0}.txt'.format(events_filename)
    ls_events = 'aws s3 ls {0} > {1}'.format(s3_path, event_txt)
    os.system(ls_events)
    event_str = open(str(event_txt), 'r').read()
    os.remove(event_txt)
    eventint = event_str.replace('PRE', '')
    events = eventint.replace('                            ', '').split('/\n')
    for i in events:
        if 'E' not in i:
            events.remove(i)
    event_json = input_dir / '{0}.json'.format(events_filename)
    with open(str(event_json), 'w') as f:
        json.dump(events, f)
    if verbose:
        print('{} events identified'.format(len(events)))
    return events


def slice_list(lst: list, n_lists: int) -> list:
    """Slices a list into n_lists number of separate lists.
    """
    n_total = len(lst)
    n_slice = int(np.round(n_total / n_lists, 0))
    slice_lst = []
    imin = 0
    imax = n_slice
    for i in np.arange(1, n_lists + 1):
        if i > 1:
            imin = imax
            imax = imin + n_slice
            if imax > n_total:
                imax = n_total
        slice_lst.append(lst[imin:imax])
    return slice_lst


def breakline_hydrographs(events: list, s3_path: str, input_dir: plib,
                          flow_export: plib, project: str, projection: plib,
                          hydro_file: str, breakline: plib, output_dir: plib,
                          verbose: bool = True) -> None:
    """For each event, this function downloads pluvial model data from AWS S3,
       runs FlowExport.exe in order to calculate the hydrograph at the
       specified breakline, and then deletes the downloaded files.
    """
    for event in events:
        start_time = time.time()
        if verbose:
            print('Calculating breakline hydrograph for event: ', event)
        s3_event_dir = '{0}{1}/'.format(s3_path, event)
        event_dir = input_dir / event
        aws_str = ('aws s3 cp {0} {1} --recursive --exclude "*" --include'
                   ' "*.prj" --include "*.hdf"'.format(s3_event_dir,
                                                       str(event_dir)))
        os.system(aws_str)
        project_path = event_dir / project
        hydro_result = output_dir / '{0}_{1}.csv'.format(hydro_file, event)
        q_export_str = '{0} {1} {2} {3} {4}'.format(flow_export,
                                                    str(project_path),
                                                    str(projection),
                                                    str(hydro_result),
                                                    str(breakline))
        os.system(q_export_str)
        shutil.rmtree(event_dir)
        if verbose:
            print('Runtime: {} Minutes\n'.format(round(time.time() -
                                                       start_time) / 60, 3))
    return None


def check_if_missing(event_files: list, input_dir: plib, elist_filename: str,
                     save_to_json: bool = True,
                     remove_intermediates=True) -> list:
    """Identifies events with missing breakline hydrographs, deletes the
       original events list, and creates a new list of events that are
       missing.
    """
    processed_events = []
    for f in event_files:
        stem = f.stem
        processed_events.append(stem.split('_')[3])
    event_json = str(input_dir / '{0}.json'.format(elist_filename))
    with open(event_json) as f:
        events = json.load(f)
    if remove_intermediates:
        os.remove(event_json)
    missing_events = list(set(events) - set(processed_events))
    n_missing = len(missing_events)
    if n_missing > 0:
        if save_to_json:
            lst_path = input_dir / '{0}_missing.json'.format(elist_filename)
            with open(str(lst_path), 'w') as f:
                json.dump(missing_events, f)
        print('{0} missing events'.format(n_missing))
    else:
        print('No missing events')
    return missing_events


def breakline_FID_BCN(breakline: plib, bcn_attribute: str = 'BCN', ) -> dict:
    """Constructs a dictionary where the keys are the feature ID and the
       values are the corresponding boundary condition names.
    """
    gdf = gpd.read_file(breakline)
    gdf['BCN_int'] = gdf[bcn_attribute].apply(lambda x: int(x.replace('Q', '')))
    gdf = gdf.sort_values(by=['BCN_int'])
    bl_dickeys = {}
    for i in gdf.index:
        bl_dickeys['feature_{}'.format(i)] = gdf.loc[i][bcn_attribute]
    return bl_dickeys


def combine_breakline_hydro(event_files: list, bl_dickeys: dict,
                            output_dir: plib, base_filename: str,
                            remove_intermediates: bool = True) -> dict:
    """Extracts the breakline hydrographs from each csv file/for each event
       and saves to a single JSON file.
    """
    bl_dic = {}
    for i, file in enumerate(event_files):
        event = file.stem.split('_')[3]
        df = pd.read_csv(file)
        idx = list(df['period'])
        for k, v in bl_dickeys.items():
            if v not in list(bl_dic.keys()):
                bl_dic[v] = {}
            bl_dic[v][event] = [idx, list(df[k])]
        if remove_intermediates:
            os.remove(file)
    with open(output_dir / '{0}_BLs.json'.format(base_filename), 'w') as f:
        json.dump(bl_dic, f)
    return bl_dic


def identify_jp_bcns(fp: pd.DataFrame, verbose: bool = True) -> list:
    """Identify the external flow boundary condition names whose joint
       probability is to be calculated.
    """
    bcns = [x for x in list(fp.index) if fp.loc[x]['Joint Probability (YES '
                                                   'or NO)'] == 'YES']
    if verbose:
        print('Calculate joint probability for: ', bcns)
    return bcns


def identify_remaining_bcns(fp: pd.DataFrame, bcns: list,
                            verbose: bool = True) -> list:
    """Identify the external flow boundary conditions whose joint probability
       is not calculated.
    """
    re_bcns = [x for x in list(fp.index) if x not in bcns]
    if verbose:
        if len(re_bcns) > 0:
            print('Remaining flow boundaries:', re_bcns)
        else:
            print('No remaining flow boundaries')
    return re_bcns


def load_normhydros(inputs_dir: plib, bcns: list, project_area: str,
                    fluvial_model: str, verbose: bool = True) -> dict:
    """Loads the normalized mean hydrographs and their associated metadata
       for each boundary condition name within the BCNs list.
    """
    dic = {}
    for bcn in bcns:
        with open(inputs_dir/'{0}_{1}_normhydro_{2}.json'.format(project_area,
                  fluvial_model, bcn)) as f:
            dic[bcn] = json.load(f)
        if verbose:
            print('Loaded normalized mean hydrograph for {0}'.format(bcn))
    return dic


def extract_fmodel_params(fm: pd.DataFrame, fmodel: str,
                          run_dur_col: str = 'Run Duration (Days)', 
                          timestep_col: str = 'Timestep (Minutes)', 
                          idx_ord_col: str = 'Time Index Ordinate',
                          verbose: bool = True) -> list:
    """Extract the fluvial model's run duration, time step, and time index
       ordinate.
    """
    run_duration = fm.loc[fmodel][run_dur_col]
    tstep = fm.loc[fmodel][timestep_col]
    idx_ord = fm.loc[fmodel][idx_ord_col]
    assert type(run_duration) == np.int64, ('Run duration is not an integer as'
                                            ' expected')
    assert type(tstep) == np.int64, 'Timestep is not an integer as expected'
    results = [int(run_duration), int(tstep), idx_ord]
    if verbose:
        print('Run duration: {0} days'.format(run_duration), '\nTimestep: {0} '
              'minutes'.format(tstep), '\nTime index ordinate: '
              '{0}'.format(idx_ord))
    print('Note: the run duration and timestep are assumed to have units of '
          'days and minutes, respectively')
    return results


def ffc_dic(bcns: list, fp: pd.DataFrame, outputs_dir: plib,
            verbose: bool = True) -> dict:
    """Construct a dictionary containing the flow frequency curve for each
       external flow boundary.
    """
    ffc = {}
    for bcn in bcns:
        gage = str(fp.loc[bcn]['Gage ID']).zfill(8)
        file = outputs_dir/'Sampler_Operation_{}.xlsx'.format(gage)
        df = pd.read_excel(file, sheet_name='Mean Curve', index_col='AEP')
        ffc[bcn] = df
        if verbose:
            print('Flow frequency curve for {0}:'.format(bcn))
            display(df.head(2))
    return ffc


def construct_jp_pairs(rname: list, verbose: bool = True) -> list:
    """Constructs a list of unique confluence pairs given a list of river
       names.
    """
    pairs = []
    for i, first in enumerate(rname[:-1]):
        for j, second in enumerate(rname[1 + i:]):
            pairs += [(first, second)]
    if verbose:
        print('Joint probability pairs:', pairs)
    return pairs


def calc_DA_total_ratio(da1: float, da2: float, verbose: bool = True) -> list:
    """Calculates the total drainage area and the ratio of the drainage
       areas. The ratio is calculated so that it is always greater than or
       equal to 1.0. The total drainage area and individual drain areas are
       passed to the check_NCHRP_assumptions function in order to test the
       assumptions of the selected joint probability procedure.
    """
    da_total = da1 + da2
    if da1 >= da2:
        da_ratio = da1 / da2
    else:
        da_ratio = da2 / da1
    if verbose:
        print('Total drainage area: {0} square miles; drainage area ratio: '
              '{1}'.format(np.round(da_total, 2), np.round(da_ratio, 2)))
    check_NCHRP_assumptions(da_total, da1, da2)
    results = [da_total, da_ratio]
    return results


def check_NCHRP_assumptions(da_total: float, da1: float, da2: float) -> None:
    """Tests two of the assumptions of the selected joint probability as
       stated on page 2 of Appendix H in "Estimating Joint Probabilities of
       Design Coincident Flows at Stream Confluences"
       (http://onlinepubs.trb.org/onlinepubs/nchrp/nchrp_w199.pdf).
    """
    if da_total >= 9000:
        warnings.warn("The sum of the watershed areas is greater than or "
                      "equal to 9000 square miles, violating a condition "
                      "of the NCHRP procedure")
    if da1 <= 1.0 or da2 <= 1.0:
        warnings.warn("One or more of the drainage areas is less than or "
                      "equal to 1 square mile, violating a condition of the "
                      "NCHRP procedure")


def watershed_category(da_ratio: float, da_total: float,
                       verbose: bool = True) -> str:
    """Determines the degree of correlation (watershed category) for the pair
       of confluent streams based on the sum of the drainage areas and the
       drainage area ratio. Values were copied from Table H.2. of "Estimating
       Joint Probabilities of Design Coincident Flows at Stream Confluences"
       (http://onlinepubs.trb.org/onlinepubs/nchrp/nchrp_w199.pdf).
    """
    if da_ratio < 7.0:
        if da_total < 350.0:
            cat_code = 'SS'
        else:
            cat_code = 'SL'
    else:
        if da_total < 350.0:
            cat_code = 'LS'
        else:
            cat_code = 'LL'
    if verbose:
        print('\nWatershed Category:', cat_code)
    return cat_code


def identify_lookup_col(ri: float, verbose: bool = True) -> str:
    """Extracts the column name corresponding to the passed recurrence
       interval.
    """
    lookup_col = 'Unavailable, check passed recurrence interval'
    if ri < 10.0:
        lookup_col = 'RI<10'
    elif ri == 10.0:
        lookup_col = 'RI=10'
    elif 10.0 < ri < 25.0:
        lookup_col = '10<RI<25'
    elif ri == 25.0:
        lookup_col = 'RI=25'
    elif 25.0 < ri < 50.0:
        lookup_col = '25<RI<50'
    elif ri == 50.0:
        lookup_col = 'RI=50'
    elif 50.0 < ri < 100.0:
        lookup_col = '50<RI<100'
    elif ri == 100.0:
        lookup_col = 'RI=100'
    elif 100.0 < ri < 500.0:
        lookup_col = '100<RI<500'
    elif ri == 500.0:
        lookup_col = 'RI=500'
    elif ri > 500.0:
        lookup_col = 'RI>500'
    if verbose:
        print("Lookup column name:", lookup_col)
    return lookup_col


def determine_joint_prob(events: pd.DataFrame, lookup: pd.DataFrame,
                         num_pairs: int, rname1: str, rname2: str,
                         cat_code: str, round_to_int: bool = False,
                         min_ri: float = 1.0, jp_main_only: bool = False,
                         verbose: bool = True) -> list:
    """Determines the joint probability for each recurrence interval (RI)
       within the events dataframe. For each RI, the lookup column is
       determined using the identify_lookup_col function, the RI is then
       divided by the number contained within the lookup table for the
       specified lookup column and watershed category, and then the resulting
       joint RI is added to the events dataframe. Lastly, the events dataframe
       is duplicated and the RIs for the confluence pairs are transposed to
       duplicate the analysis for the transposed confluence pair.
    """
    riname1 = 'RI_{0}'.format(rname1)
    riname2 = 'RI_{0}'.format(rname2)
    df = events.copy(deep=True)
    if jp_main_only:
        pair_multiplier = 1.0
    else:
        pair_multiplier = 2.0
    df['Weight'] = df['Weight'] / (num_pairs * pair_multiplier)
    if round_to_int:
        df['RI'] = df['RI'].apply(lambda x: np.round(x))
    df = df.rename(columns={'RI': riname1})
    ri2_lst = []
    for i in df.index:
        ri1 = df.loc[i][riname1]
        lookup_col = identify_lookup_col(ri1, False)
        ri2 = ri1 / lookup.loc[cat_code][lookup_col]
        ri2_lst.append(ri2)
    df[riname2] = ri2_lst
    if round_to_int:
        df[riname2] = df[riname2].apply(lambda x: np.round(x))
    else:
        df[riname1] = df[riname1].apply(lambda x: np.round(x, 2))
        df[riname2] = df[riname2].apply(lambda x: np.round(x, 2))
    df[riname1] = [x if x >= min_ri else min_ri for x in list(df[riname1])]
    df[riname2] = [x if x >= min_ri else min_ri for x in list(df[riname2])]
    jptable1 = df.copy(deep=True)
    if jp_main_only:
        results = [jptable1]
        print('Joint probability only calculated for the first combination '
              'in Tables H.3. to H.7.')
    else:
        jptable2 = df.rename(columns={riname1: riname2, riname2: riname1})
        cols = ['Weight', riname1, riname2]
        jptable2 = jptable2[cols]
        results = [jptable1, jptable2]
        print('\nJoint probability table for {0}_{1}:'.format(rname1, rname2))
    if verbose:
        print(display(jptable1.head(2)))
    return results


def joint_prob_tables(bcns: list, fp: pd.DataFrame, events: pd.DataFrame,
                      lookup: pd.DataFrame, min_ri: float = 1.01,
                      da_col: str = 'Drainage Area (miles^2)',
                      round_to_int: bool = False, jp_main_only: bool = False,
                      verbose: bool = True) -> list:
    """Calculates a table of joint probabilities (return periods) for
       confluence pair.
    """
    pairs = construct_jp_pairs(bcns, verbose)
    num_pairs = len(pairs)
    jptables = []
    for pair in pairs:
        da1 = fp.loc[pair[0]][da_col]
        da2 = fp.loc[pair[1]][da_col]
        da_total, da_ratio = calc_DA_total_ratio(da1, da2, verbose)
        cat_code = watershed_category(da_ratio, da_total, verbose)
        if da1 >= da2:
            jptables.append(determine_joint_prob(events, lookup, num_pairs,
                                                 pair[0], pair[1], cat_code,
                                                 round_to_int, min_ri,
                                                 jp_main_only, verbose))
        else:
            jptables.append(determine_joint_prob(events, lookup, num_pairs,
                                                 pair[1], pair[0], cat_code,
                                                 round_to_int, min_ri,
                                                 jp_main_only, verbose))
    return jptables


def joint_prob_q(jplst: list, ffc: dict,
                 verbose: bool = True) -> pd.DataFrame:
    """Calculate the discharge for each recurrence interval in the joint
       probability tables using the boundary condition-specific flow
       frequency curves.
    """
    table_lst = []
    for pair in jplst:
        for table in pair:
            fullnames = [x for x in list(table.columns) if 'RI_F' in x]
            names = [x.split('_')[1] for x in fullnames]
            for i, name in enumerate(names):
                ffc_aepz = zvar(list(ffc[name].index))
                ffc_logq = np.log10(ffc[name]['Q_Mean_cfs'])
                f = mean_q(ffc_aepz, ffc_logq)
                aepz = zvar([1 / x for x in list(table[fullnames[i]])])
                table['Q_{}'.format(name)] = [int(x) for x in 10 ** f(aepz)]
            table_lst.append(table.reset_index())
    combined = pd.concat(table_lst, axis=0, sort=True)
    combined = combined.sort_values(by=['Weight'],
                                    ascending=False).reset_index(drop=True)
    combined['Events'] = ['E{}'.format(str(x + 1).zfill(4)) for x in list(combined.index)]
    combined = combined.set_index('Events')
    cols = list(combined.columns)
    col_order = ['Weight'] + [x for x in cols if 'RI' in x] + [x for x in cols if 'Q' in x]
    combined = combined[col_order]
    if verbose:
        display(combined.head(2))
    return combined


def add_remaining_bcns(re_bcns: list, fp: pd.DataFrame,
                       jptable_q: pd.DataFrame, perc_mean: float,
                       verbose: bool = True) -> pd.DataFrame:
    """Add the remaining external flow boundaries conditions to the joint
       probability table using a constant percentage of their mean discharge.
    """
    for bcn in re_bcns:
        q = fp.loc[bcn]['Annual Mean (cfs)']*perc_mean
        jptable_q['Q_{}'.format(bcn)] = [int(x) for x in np.ones(len(jptable_q))*q]
    if verbose:
        display(jptable_q.head(2))
    return jptable_q


def build_fluvial_dict(run_dur: int, tstep: int, idx_ord: str, 
                       fluvial_bc_units: str, verbose: bool = True) -> dict:
    """Build a dictionary to store the fluvial forcing data for each boundary
       condition name, along with the associated metadata. 
    """
    run_dur_hr = run_dur*24.0
    tstep_hr = tstep/60.0
    idx = list(np.arange(0, run_dur_hr, tstep_hr)) + [run_dur_hr]
    ff_dic = {'Fluvial': {}}
    ff_dic['Fluvial']['time_idx_ordinate'] = idx_ord
    ff_dic['Fluvial']['run_duration_days'] = run_dur
    ff_dic['Fluvial']['time_idx'] = idx
    ff_dic['Fluvial']['fluvial_BC_units'] = fluvial_bc_units
    ff_dic['Fluvial']['BCName'] = {}
    if verbose:
        print('Dictionary structure: \n {0}\n  {1}'
              ''.format(list(ff_dic.keys()), list(ff_dic['Fluvial'].keys())))
    return ff_dic


def append_norm_hydros(ff_dic: dict, mean_x: dict, mean_y: dict,
                       q_mean: pd.DataFrame, data_threshold: float,
                       bcn: str) -> dict:
    """Add the normalized mean hydrographs for each event in the q_mean
       dataframe to the passed fluvial forcing dictionary.
    """
    idx = ff_dic['Fluvial']['time_idx']
    ff_dic['Fluvial']['BCName'][bcn] = {}
    for i in q_mean.index:
        ri = q_mean.loc[i, 'RI']
        x = list(mean_x[ri])
        y = list(mean_y[ri])
        f = interp1d(x, y, kind='cubic')
        x_trunc = [i for i in idx if i < max(x)]
        y_fit = list(f(x_trunc))
        y_pad = list(np.ones(len(idx) - len(x_trunc)) * data_threshold)
        ff_dic['Fluvial']['BCName'][bcn][i] = y_fit + y_pad
    return ff_dic


def calc_norm_hydros(jptable_q: pd.DataFrame, uhydro: dict, bcn: str,
                     idx: list, verbose: bool = True) -> dict:
    """Calculate the unit hydrographs for each event within the jptable_q
       dataframe for the specified boundary condition name.
    """
    df = jptable_q.drop(columns=[x for x in jptable_q.columns 
                                 if bcn not in x]).copy(deep=True)
    df = df.rename(columns={'Q_{0}'.format(bcn): 'Q_Mean_cfs', 
                            'RI_{0}'.format(bcn): 'RI'})
    df1 = df.drop_duplicates(subset='RI')
    data_threshold = uhydro['data_threshold']
    selected_bins = uhydro['selected_bins']
    selected_peaks = {int(k): v for (k, v) 
                      in uhydro['selected_peaks'].items()}
    dfslice = pd.DataFrame(uhydro['dfslice']).reset_index()
    dfslice['index'] = [int(x) for x in dfslice['index']]
    dfslice = dfslice.set_index('index')
    dfslice['increment_hr'] = [timedelta(seconds=x) for x 
                               in list(dfslice['increment_hr'])]
    bins_avg_cum_hr = uhydro['bins_avg_cum_hr']
    events_int = uhydro['events']
    int_increment = uhydro['int_increment']
    excluded_dates = uhydro['excluded_dates']
    results = save_plot_preprocessing(data_threshold, selected_bins, 
                                      selected_peaks, dfslice, df1, 
                                      bins_avg_cum_hr, events_int, 
                                      int_increment, excluded_dates)
    bin_return_pd, hydro_x, hydro_y, flow_df_normailzed = results
    dic = {}
    for e in df.index:
        ri = df.loc[e]['RI']
        x = list(hydro_x[ri])
        y = list(hydro_y[ri])
        f = interp1d(x, y, kind='cubic')
        x_trunc = [i for i in idx if i < max(x)]
        y_fit = list(f(x_trunc))
        dic[e] = y_fit + list(np.ones(len(idx)-len(x_trunc))*data_threshold)
    if verbose:
        print('Normalized mean hydrographs calculated for {0}'.format(bcn))
        minimum = df['Q_Mean_cfs'].min()
        maximum = df['Q_Mean_cfs'].max()
        min_event = df[df['Q_Mean_cfs'] == minimum].index[0]
        max_event = df[df['Q_Mean_cfs'] == maximum].index[0]
        hydro_min = int(max(dic[min_event]))
        hydro_max = int(max(dic[max_event]))
        print('Original peak min: {0}, max: {1}\nHydrograph peak min: {2}, '
              'max: {3}\n'.format(minimum, maximum, hydro_min, hydro_max))
    return dic


def build_fluvial_weights_dic(domain: str, events: pd.DataFrame) -> dict:
    """Build a dictionary to store the fluvial events and their corresponding
       weights.
    """
    weights_dic = {'BCName': {domain: {}}}
    weights = []
    for e in events.index:
        weight = events.loc[e]['Weight']
        weights_dic['BCName'][domain][e] = weight
        weights.append(weight)
    t = np.round(sum(weights), 5)
    assert t == 0.5, 'Weights add to {0} instead of 0.5'.format(t)
    return weights_dic


def pad_pluvial_forcing(file: plib, plen: int, uniform_plen: bool = True, 
                        verbose: bool = True) -> None:
    """Open the json file, pad the time index and the pluvial forcing data,
       and overwrite the original json file.
    """
    with open(file) as f:
        f_dic = json.load(f)
    updated_dic = {}
    for d in list(f_dic.keys()):
        updated_dic[d] = {}
        for k, v in f_dic[d].items():
            run_dur = f_dic[d]['run_duration_days']
            run_dur_hr = run_dur*24.0
            idx = f_dic[d]['time_idx']
            tstep = idx[1] - idx[0]
            start = idx[-1] + tstep
            if not uniform_plen:
                plen = int((run_dur_hr-start)/tstep)+1
            if k == 'time_idx':
                updated_dic[d][k] = idx + list(np.arange(start, 
                                               start+plen*tstep, tstep))
            elif k == 'BCName':
                bcns = list(f_dic[d][k].keys())
                updated_dic[d][k] = {}
                for b in bcns:
                    updated_dic[d][k][b] = {}
                    for e, vals in f_dic[d][k][b].items():
                        updated_dic[d][k][b][e] = vals + list(np.zeros(plen))
            else:
                updated_dic[d][k] = v       
    with open(file, 'w') as f:
        json.dump(updated_dic, f)
    if verbose:
        print('Updated: {0}'.format(file.stem))   


def calc_uniform_hydros(jptable_q: pd.DataFrame, bcn: str, idx: list,
                        verbose: bool = True) -> dict:
    """Calculate uniform hydrographs for each event within the jptable_q
       dataframe for the specified boundary condition name.
    """
    dic = {}
    col = 'Q_{0}'.format(bcn)
    for e in jptable_q.index:
        dic[e] = list(np.ones(len(idx))*jptable_q.loc[e][col])
    if verbose:
        print('Uniform hydrographs calculated for {0}'.format(bcn))
    return dic


def zvar(cl: list) -> np.ndarray:
    """Used to calculate the standard normal z variate of the passed
       confidence limit.
    """
    clz = np.zeros(len(cl))
    for i in np.arange(len(cl)):
        clz[i] = norm.ppf((1 - cl[i]))
    return clz


def mean_q(aepmz: np.ndarray, q: list):
    """Creates a function for calculating the mean log flow for each AEP.
    """
    f = interp1d(aepmz, q, fill_value='extrapolate')
    return f


def monotonic_test(df: pd.DataFrame, adj_amount: float = 0.1) -> pd.DataFrame:
    """Test that the discharge decreases with increasing annual exceedance
       probability and adjust the discharge at the smallest annual exceedance
       probabilities if not.
    """
    for col in df.columns:
        if np.diff(df[col]).max() >= 0:
            maximum = df[col].max()
            idx = df[col].idxmax()
            diff = np.around(maximum - df.iloc[0][0], 1)
            adj_df = df.loc[:idx][col]
            num = adj_df.shape[0]
            val = np.arange(maximum, maximum + num * adj_amount, adj_amount)
            for i, v in enumerate(adj_df.index):
                df.loc[v][col] = val[num - 1 - i]
            message = 'Q not decreasing with increasing AEP for the'
            cl = float(col) * 100.0
            aep = df.iloc[0].name
            solution = 'Adjusting Q'
            warnings.warn("{0} {1}% CL: difference of {2} cfs between {3} and"
                          " {4}. {5}".format(message, cl, diff, aep, idx,
                                             solution))
    return df


# noinspection DuplicatedCode
def plot_breakline_ffc(df: pd.DataFrame, gage: str) -> None:
    """Use the results of PeakFQ to plot the breakline's flow frequency
       curve.
    """
    fig, ax = plt.subplots(1, 1, figsize=(12, 6))
    ax.plot(df.index.values * 100.0, df['Q_cfs'], marker='', label=gage,
            color='black')
    ax.set_xscale('log')
    ax.set_xlim([100, 0.15])
    ax.invert_xaxis()
    ax.set_xlabel('Annual exceedance probability, [%]')
    ax.set_ylabel('Discharge, [cfs]')
    ax.xaxis.set_major_formatter(mpl.ticker.FormatStrFormatter('%d'))
    ax.set_title('Flow Frequency Curve')
    ax.legend()
    ax.grid(True, which="both")
    return None


def plot_peakq(input_dic: dict, xmax: int = 90, ymax: int = 40000,
               legend: bool = True) -> None:
    """Plot the peak streamflow for each gage within the passed dictionary.
    """
    fig, ax = plt.subplots(1, 1, figsize=(12, 6))
    for k, v in input_dic.items():
        ax.plot(v.index.values, v['Q_cfs'], linestyle='', marker='.', label=k)
    ax.set_xlim([0, xmax])
    ax.set_ylim([0, ymax])
    ax.set_xlabel('Event Number')
    ax.set_ylabel('Discharge, [cfs]')
    ax.set_title('Peak Streamflow (Sorted)')
    if legend:
        ax.legend()
    ax.grid(True, which="both")
    return None


# noinspection DuplicatedCode
def plot_ffc_lin_reg(output_dic: dict, gagestats: dict = None,
                     ymax: int = 50000, logx: bool = True) -> None:
    """Plot the flow frequency curve and, optionally, the linear regression
       calculated using the calc_ffc_slope function for each gage.
    """
    fig, ax = plt.subplots(1, 1, figsize=(12, 6))
    for k, v in output_dic.items():
        ax.plot(v.index.values*100.0, v['Q_cfs'], marker='', label=k)
        if gagestats:
            ax.plot(gagestats[k]['x']*100.0, gagestats[k]['ypre'],
                    color='black')
    if logx:
        ax.set_xscale('log')
    ax.set_xlim([0.1, 100.0])
    ax.invert_xaxis()
    ax.set_ylim([0, ymax])
    ax.set_xlabel('Annual exceedance probability, [%]')
    ax.set_ylabel('Discharge, [cfs]')
    ax.set_title('Flow Frequency Curve')
    ax.xaxis.set_major_formatter(mpl.ticker.FormatStrFormatter('%d'))
    ax.legend()
    ax.grid(True, which="both")
    return None


def plot_breakline_hydro(results_dic: dict, resample_events: int = 1,
                         resample_ts: int = 12,
                         plot_legend: bool = False) -> None:
    """For each breakline, plot the breakline hydrograph for select
       events and with a resampled timestep.
    """

    for bcn, e_dic in results_dic.items():
        fig, ax = plt.subplots(figsize=(30, 8))
        count = 0
        for event, values in e_dic.items():
            resampled_values = []
            resampled_idx = []
            if (count / resample_events).is_integer():
                for i, item in enumerate(values[1]):
                    if (i / resample_ts).is_integer():
                        resampled_values.append(item)
                        resampled_idx.append(values[0][i])
                ax.plot(resampled_idx, resampled_values, label=event)
            count += 1
        ax.set_xlabel('Datetime')
        ax.set_ylabel('Discharge, [cfs]')
        if plot_legend:
            ax.legend()
        plt.xticks(rotation=45)
        ax.set_title('{} Breakline Hydrograph'.format(bcn))
    return None


def plot_normalized_uniform_hydro(ff_dic: dict, project_area: str,
                                  fluvial_model: str) -> None:
    """Plot both the normalized mean and uniform hydrographs contained within
       the passed fluvial forcing dictionary. 
    """
    bcns = list(ff_dic['Fluvial']['BCName'].keys())
    num = len(bcns)
    idx = ff_dic['Fluvial']['time_idx']
    idx_ord = ff_dic['Fluvial']['time_idx_ordinate']
    units = ff_dic['Fluvial']['fluvial_BC_units']
    fig, ax = plt.subplots(num, 1, figsize=(24, num*6))
    for i, bcn in enumerate(bcns):
        if num > 1:
            for k, v in ff_dic['Fluvial']['BCName'][bcn].items():
                ax[i-1].plot(idx, v)
            ax[i-1].set_ylabel('Discharge, [{0}]'.format(units))
            ax[i-1].set_title('{0} {1} Forcing - {2} Hydrographs'
                              ''.format(project_area, fluvial_model, bcn))
            ax[i-1].set_ylim(bottom=0)
            ax[num-1].set_xlabel('Time, [{0}]'.format(idx_ord))
        elif num == 1:
            for k, v in ff_dic['Fluvial']['BCName'][bcn].items():
                ax.plot(idx, v)
            ax.set_ylabel('Discharge, [{0}]'.format(units))
            ax.set_title('{0} {1} Forcing - {2} Hydrographs'
                         ''.format(project_area, fluvial_model, bcn))
            ax.set_ylim(bottom=0)
            ax.set_xlabel('Time, [{0}]'.format(idx_ord))


def binQ(df1: pd.DataFrame) -> np.ndarray:
    """Determines the minimum and maximum discharge value for the passed
       dataframe and constructs an array of equally spaced discharge between
       these two values.
    """
    qmin = df1.min().min()
    qmax = df1.max().max()
    q = np.linspace(qmin, qmax, num=len(df1))
    return q


def interp_AEP(df1: pd.DataFrame, q: np.ndarray, clz: np.ndarray,
               aepz: np.ndarray, extrapolate: bool = True) -> pd.DataFrame:
    """Apply linear interpolation/extrapolation to calculate AEP(z) for
       each CL(z) and binned flow.
    """
    df2 = df1.copy(deep=True)
    df2['Q'] = q
    df2.set_index('Q', inplace=True)
    df2.columns = clz
    if not extrapolate:
        for j in np.arange(len(clz)):
            f = interp1d(df1.iloc[:, j], aepz)
            for i in np.arange(len(q)):
                if q[i] < df1.iloc[:, j].min():
                    df2.iloc[:, j][i] = aepz[-1]
                if (q[i] >= df1.iloc[:, j].min()) & (q[i] <= df1.iloc[:, j].max()):
                    df2.iloc[:, j][i] = f(q[i])
                if q[i] > df1.iloc[:, j].max():
                    df2.iloc[:, j][i] = aepz[0]
    if extrapolate:
        for j in np.arange(len(clz)):
            f = interp1d(df1.iloc[:, j], aepz, fill_value='extrapolate')
            for i in np.arange(len(q)):
                df2.iloc[:, j][i] = f(q[i])
    return df2


# NEEDS FORMATTING BELOW ----------------------------------------------------
def zvar_inv(df2,
             CL):  # Calculate the the inverse of the standard normal Z variate for each AEPz. Note the inverse of the standard normal Z variate of CLz is just CL which was already calcualted
    df2.columns = CL
    df3 = df2.copy(deep=True)
    for i in np.arange(
            len(df2.iloc[0])):  # Take the inverse of the standard normal z variate for each AEPz calcualted in step 5
        df3.iloc[:, i] = 1 -  norm.cdf(df2.iloc[:, i])
    return df3


def mean_AEP(df3: pd.DataFrame) -> list:
    """Calculate the mean (expected) value of the AEP for each flow; this
       mean is equal to the area under the CDF calculated using the
       trapezoidal rule"""
    AEPm = []
    for i in np.arange(len(df3.index)):
        x = df3.columns.values
        y = df3.iloc[i]
        leftside = df3.iloc[i, 0] * df3.columns[0]
        rightside = df3.iloc[i, -1] * df3.columns[0]
        AEPm.append(trapz(y, x=x) + leftside + rightside)
    return AEPm


def median_Q(AEPz, df1):  # Create a function for interpolating the median log flow for each AEP
    f1 = interp1d(AEPz, df1.iloc[:]['0.5'], fill_value='extrapolate')
    return f1


def ffc_summary(AEP1, AEP1z):  # Create a summary table of the median and mean flow frequency curves
    df4 = pd.DataFrame(data={'AEPz': AEP1z}, index=AEP1)
    df4.index.name = 'AEP'
    return df4


def SSdata(f):  # Function to load data from the USGS StreamStats Output File
    with open(f) as fin:
        lines = fin.readlines()
        for i, line in enumerate(lines):
            if 'Peak-Flow Statistics Flow Report,Area-Averaged\n' in line:
                skiprow = i + 2
            if '"500 Year Peak Flood"' in line:
                endrow = i
    df = pd.read_csv(f, skiprows=skiprow, nrows=(endrow - skiprow + 1), usecols=[0, 1, 3],
                     names=['Recurrance Interval', 'Discharge',
                            'Equiv. Yrs.'])  # Create the dataframe using the data from StreamStats. Specify the number of rows to skip, the number of rows to keep, the columns to keep, and the names of the columns
    df['Recurrance Interval'] = df['Recurrance Interval'].map(lambda x: float(x.rstrip(
        'Year Peak Flood')))  # Right strip the string "Year Peak Flood" from the Return inteval column and make it a flaot
    df.set_index('Recurrance Interval', inplace=True)
    return df


def fun_AEP_Q(df, AEP, deg):
    AEPz = zvar(AEP)  # Transform this AEP to the standard normal z variate

    interp = ffc_summary(AEP, AEPz)  # Construct a table to store the results of the interpolation functiojn

    a, b, c = np.polyfit(df.AEPz, df.Qlog, deg=deg)  # Second order polynomial

    interp['Qlog_int'] = a * AEPz ** 2 + b * AEPz + c
    interp['Q_int'] = 10 ** interp['Qlog_int']
    return interp


def RI(min_val: float, max_val: float, nbin: int) -> pd.DataFrame:
    """Divide the log range between the minimum and maximum recurrance
       interval into equal increments.
    """
    steps = 10 ** np.linspace(np.log10(min_val), np.log10(max_val), nbin + 1)
    RI = 1 / ((1 / steps[0:nbin] + 1 / steps[1:nbin + 1]) / 2.0)
    df1 = pd.DataFrame(data={'Center': RI})
    return df1


def AEP_weights(df1: pd.DataFrame, min_val: float, max_val: float) -> pd.DataFrame:
    """
    """
    df1['Ceiling'] = np.zeros(len(df1))
    df1['Floor'] = np.zeros(len(df1))
    df1['Ceiling'].iloc[0] = 2.0
    for i in np.arange(len(df1) - 1):
        df1['Floor'].iloc[i] = (df1['Center'].iloc[i] + df1['Center'].iloc[i + 1]) / 2
    df1['Floor'].iloc[-1] = max_val
    for i in np.arange(1, len(df1)):
        df1['Ceiling'].iloc[i] = df1['Floor'].iloc[i - 1]
    df2 = df1.rdiv(
        1)  # Take the inverse of all of the entire dataframe so that the ceiling and floor values correspond to the annual exceedance probabilities
    df2['Weight'] = df2['Ceiling'] - df2[
        'Floor']  # Calculate the weight of the bin by taking the difference between the ceiling and the floor, i.e. how much of the probability corresponds to that bin
    total = sum(df2['Weight'])  # Calculate the sum of all of the weights, this should be approximately 0.5
    df2['Weight'].iloc[0] += (0.5 - total)  # Add the extra weight to the first weight
    df2 = df2.set_index('Center')  # Set the index to the center value
    df2.index.name = 'AEP'  # Rename the index the annual exceedance probability since that it was it corresponds to for each bin (the maximum value of AEP)
    print(display(df2.head(2)), 'Sum of weights:', sum(df2['Weight']))
    return df2


def add_events(df: pd.DataFrame) -> pd.DataFrame:
    """Add an events column to the dataframe and set it as the index.
    """
    Events = []
    df = df.reset_index()
    for i in df.index:
        Events.append('E{0}'.format(str(i + 1).zfill(4)))
    df['Events'] = Events
    df = df.set_index('Events')
    df.index.name = ''
    print(display(df.head(2)))
    return df


def format_mean_curve(mc_dict):
    """ Function used to convert the mean curve's dictionary into a dataframe
    """
    AEPs = [float(i) for i in
            list(mc_dict['Q_Mean_cfs'].keys())]  # Convert the Annual exceedance probabilities to floats
    mc = pd.DataFrame(data={'Q_Mean_cfs': list(mc_dict['Q_Mean_cfs'].values()),
                            'Q_Median_cfs': list(mc_dict['Q_Median_cfs'].values())}, index=AEPs)
    mc.index.name = 'AEP'
    mc = mc.sort_values('AEP', ascending=False)
    return mc


def eventIDs(table, runs,
             nevents):  # If the nevents is more than 5, the bounds within the function may have to be adjsuted
    RI = table.RI  # Extract the return periods from the dataframe
    dic = {}  # Empty dictionary
    for i in table.RI:  # Extract the mean value for each return period and add it to a dictionary
        dic[i] = table[table.RI == i]['Q_Mean_cfs'].values[0]
    df4 = runs.copy(deep=True)  # Copy the runs dataframe which has the peak discharge for each event
    df4 = df4.set_index('Event')  # Reindex the dataframe so that the index is the "Event" ID
    lower, upper = 0.80, 1.20  # The bounds which will be used to search for the nearest event discharge to the mean discharge
    dic2 = {}  # Empty dictionary
    for i in table.RI:  # Extract a list of the event IDs for the events at each RI whose discharge is closest to the mean
        dic2[i] = list(np.abs(
            df4[df4['Final Peak Flow'].between(dic[i] * lower, dic[i] * upper)]['Final Peak Flow'] - dic[
                i]).sort_values(ascending=True)[0:nevents].keys())
    return dic2


def eventtable(table, dic2):
    RI = []  # Empty list to store the recurrance itnerval
    Events = []  # Empty list to store the event IDs
    for j in dic2.keys():  # Store the recurrance intervals and the event IDs to lists
        for i in np.arange(len(dic2[j])):
            RI.append(j)
            Events.append(dic2[j][i])
    events = pd.DataFrame(data={'Events': Events, 'RI': RI})  # Add these lists to a new dataframe
    events.head()
    return events


def eventQ(events, runs, dic2, nevents):
    names = []  # Empty list to store the column name which is value1 through valuex, where x is the number of events used in this analysis.
    for i in range(0, nevents):  # add the event count to the "value" prefix.
        names.append('Value' + str(i + 1))
    col = dict.fromkeys(names)  # Make a dictionary called "col" where the keys are the names defined above.
    df4 = runs.copy(deep=True)  # Copy the runs dataframe which has the peak discharge for each event
    df4 = df4.set_index('Event')  # Reindex the dataframe so that the index is the "Event" ID
    for j in range(0, nevents):
        lst = []
        for i in events.RI:
            lst.append(df4[df4.index == dic2[i][j]]['Final Peak Flow'].values[
                           0])  # Extract the discharge for the events whose discharge is the closest to the mean discharge at that RI
        col[names[
            j]] = lst  # add the list of discharges for each of the event counts to its respective key within the col dictionary.
    summary = pd.DataFrame(col,
                           index=events.RI)  # Create  summary table where the index is the recurrance interval and the columns are the discharge values who are closest to the mean
    summary.index.name = 'RI'  # Rename the index so that it is the recurrance interval
    dropdup = summary.drop_duplicates()  # Drop any duplicate values
    dropdup[
        'AEP'] = 1 / dropdup.index * 100.0  # Convert the return interval to annual exceedance probability as a percentage
    return dropdup


def eventweight(events, df3, nevents):
    events['weight'] = np.zeros(len(events))  # Create a column in the events dataframe to store the weights
    i, j = 0, 0  # Set the counting variabiles to zero to start
    for i in events.index:  # Set the weight for each event equal to 1/2 (where 2 is the number of events for each recurrance interval) of the entire weight for the corresponding recurrance interval
        if np.around(events.RI[i], 10) == np.around(1.0 / df3.index[j],
                                                    10):  # Note I had to round the recurrance intervals since they are floats and are not exactly the same
            events['weight'][i] = df3.weight.iloc[j] / nevents
        else:
            j += 1
            events['weight'][i] = df3.weight.iloc[j] / nevents
    return events


def basebreach_format(df4):
    df4[['RunType', 'Events']] = df4['Run'].str.split('_E',
                                                      expand=True)  # Parse the run column into "Run" and "Event" columns using the underscore and remove the "E"
    df4['Events'] = pd.to_numeric(df4[
                                      'Events'])  # Convert the Event column from string to integer so that it can be compared to the event numbers whose discharge is closest to the mean for each recurrance interval
    Type = []  # Empty list to store the run type (base or breach) without the breach number
    for j in np.arange(len(df4)):
        Type.append(''.join([i for i in df4.RunType[j] if not i.isdigit()]))  # Strip the breach number
    df4['Type'] = Type  # Add the run type (base or breach) to the dataframe
    return df4


def basebreach(events, df4, Event, Eventsub):
    wtotal = []  # Empty list to store the total weight for each event ID
    for i in np.arange(len(Event)):
        wtotal.append(
            np.sum(df4[df4['Events'] == Event[i]]['Weight'].values))  # Calculate the total weight for each event ID
    df5 = df4[df4['Events'].isin(Eventsub)].copy(
        deep=True)  # Subset the Augusta dataframe to include only the events whose discharge is near the mean for a specific recurrance interval
    df5['wtotal'] = [wtotal[i] for i in df5['Events'] - 1]  # Add in the total weight for each event
    df5['prob'] = np.zeros(len(df5))  # Add a column to store the probability of each scenario
    df5['wadjust'] = np.zeros(len(df5))
    for i in df5.index:
        df5.prob[i] = df5.Weight[i] / df5.wtotal[i]  # Calculate the probability of each scenario
        df5['wadjust'][i] = df5.prob[i] * sum(events[events.Events == df4.Events[
            i]].weight.values)  # Calculate the adjusted weight for each base/breach scenario
    print(sum(df5.wadjust))
    return df5


def breachnorm(df5, Eventsub):
    wadjusttotal = []  # Empty list to store the total adjusted weight for each event ID
    for i in np.arange(len(Eventsub)):
        wadjusttotal.append(np.sum(df5[df5['Events'] == Eventsub[i]][
                                       'wadjust'].values))  # Calculate the total adjusted weight for each event ID
    df6 = df5[df5.Type == 'Breach'].copy(deep=True)  # Truncate the dataframe so that it only includes breach events
    df6['adj_weight'] = df6['wadjust'] / 5.0  # Divide the weight by 5 and add it to the adjusted weight column
    adj_breach = []  # Empty list to store the sum of the adjusted weights for each event ID
    for i in np.arange(len(Eventsub)):  # Calculate the sum of the adjuested weights for each event ID
        adj_breach.append(np.sum(df6[df6['Events'] == Eventsub[i]]['adj_weight'].values))
    adj_base = np.array(wadjusttotal) - np.array(
        adj_breach)  # Subtract the sum of the adjusted weights for the breach events from the total weight initial weight for each event to get the adjusted weight of the base
    df7 = df6.drop(['Weight', 'RunType', 'Type', 'wtotal', 'prob', 'wadjust'], axis=1).copy(
        deep=True)  # Drop the weight, runtype, and type columns since they were only needed for making calculations
    Run = []  # Empty list to store the run names for the Base scenario
    for i in Eventsub:  # For each event, create a base scenario
        Run.append('Base_E' + str(i).zfill(3))
    df8 = pd.DataFrame(data={'Run': Run, 'Events': Eventsub,
                             'adj_weight': adj_base})  # Construct a dataframe with the base scenarios, the corresponding event numbers, and the adjusted weight
    df9 = pd.concat([df7, df8]).sort_values(by=['Events']).drop(['Events'],
                                                                axis=1)  # Combine the breach and base scenario dataframes, sort by the event number, and then drop this column.
    df9 = df9.set_index('Run')  # Set the index to the Run column
    df9 = df9.rename(columns={"adj_weight": "Weight"})  # Rename the adjsuted weight column
    print(display(df9.head()), sum(df9.Weight))
    return df9


def GetFreqCurves(f, version=2_2):
    """
    GetFreqCurves reads in the raw output of HEC-SSP (.rpt) files. Ouput is slightly different
    for version 2.1 and 2.2 so a version variable was added. (e.g. 2_2 for version 2.2.x)
    """

    line_breaks_v2_1 = [(1, 13), (14, 26), (27, 40), (41, 53), (54, 65)]  # Version 2.1
    line_breaks_v2_2 = [(1, 15), (16, 29), (34, 42), (46, 60), (61, 74)]  # Version 2.2

    if version == 2_2:
        line_breaks = line_breaks_v2_2
    elif version == 2_1:
        line_breaks = line_breaks_v2_1
    else:
        print('Only works for 2.1 or 2.2')
        return None

    read_results = False
    frequencies = 0
    AEP = []
    with open(f) as fin:
        lines = fin.readlines()

        for i, line in enumerate(lines):

            if 'Upper Confidence Level:' in line:
                high = float(line.split(':')[1].replace('\n', ''))

            if 'Lower Confidence Level:' in line:
                low = float(line.split(':')[1].replace('\n', ''))

            if 'Frequency:' in line:
                AEP.append(float(line.split(':')[1].replace('\n', '')) / 100.0)

            if 'Final Results' in line:
                read_results = True

            elif '<< Frequency Curve >>' in line and read_results:
                skiprows = i + 7

            elif line[
                 0:9] == 'Frequency':  # This should be able to replace the trim rows part that was defined below I believe
                frequencies += 1

    assert (float(high) + float(
        low)) == 1.0, 'In {0} the upper and lower confidence limit value do not add to 1.0, check the user defined confidence limits in HEC-SSP'.format(
        f)

    df = pd.read_fwf(f, skiprows=skiprows, colspecs=line_breaks,
                     names=['0.5', 'Variance', 'AEP', '{}'.format(low), '{}'.format(high)])

    # Trim rows from bottom of table 
    # for idx, pct_chance in enumerate(df['Percent Chance Exceedence']):
    #    try:
    #        isnumber = float(pct_chance)
    #    except:
    #        dropline = idx
    #        break

    df = df[
         0:frequencies].copy()  # Trim the dataframe so that it is only the length of the number of rows with defined frequencies
    df = df[['0.5', 'AEP', '{}'.format(low),
             '{}'.format(high)]].copy()  # Trim the dataframe to only include the 0.5, AEP, lower CL, and upper CL

    str2num = lambda x: float(x.replace(',', ''))

    for col in df:
        try:
            df[col] = pd.to_numeric(df[col].apply(str2num))
        except:
            print('Error ...')
    df['AEP'] = AEP

    return df, high, low


def make_ssp_table(ssp_results, version=2_2):
    for i, f in enumerate(ssp_results):
        if i == 0:
            df, high, low = GetFreqCurves(f, version=version)
            df = df[['0.5', 'AEP', '{}'.format(low), '{}'.format(high)]].copy()
        else:
            tmp, high, low = GetFreqCurves(f, version=version)
            tmp = tmp[['{}'.format(low), '{}'.format(high)]].copy()
            df = df.merge(tmp, left_index=True, right_index=True)
    df = df.set_index('AEP')
    df = df.reindex(sorted(df.columns), axis=1)  # Sort the column names so that they are in numerical order

    return df


# ---------------------------------------------------------------------------#
# Plotting Functions
# ---------------------------------------------------------------------------#

'''Plotting functions called by...
'''


# ---------------------------------------------------------------------------#


def plot_ssp_curves(df):
    fig, ax = plt.subplots(figsize=(24, 10))
    for col in df.columns:
        if col == 50.0:
            ax.semilogx(df.index, df[col], label='Median Value'.format(col), color='black', linewidth=4)
        else:
            ax.semilogx(df.index, df[col], label='{} CL'.format(col))
        ax.set_title('Selected Confidence Limits\n(Generated using HEC-SSP)')
        ax.set_xlabel('Return Period')
        ax.set_ylabel('Peak Flow (CFS)')

    # ax.scatter(rand_flow_data.index/100,rand_flow_data.values )
    fig.gca().invert_xaxis()
    ax.legend();
    ax.grid()


def plot_ssp_curves(df):
    fig, ax = plt.subplots(figsize=(24, 10))
    for col in df.columns:
        if col == 50.0:
            ax.semilogx(df.index, df[col], label='Median Value'.format(col), color='black', linewidth=4)
        else:
            ax.semilogx(df.index, df[col], label='{} CL'.format(col))
        ax.set_title('Selected Confidence Limits\n(Generated using HEC-SSP)')
        ax.set_xlabel('Return Period')
        ax.set_ylabel('Peak Flow (CFS)')

    # ax.scatter(rand_flow_data.index/100,rand_flow_data.values )
    fig.gca().invert_xaxis()
    ax.legend();
    ax.grid()


# Functions added by sputnam:
def plot_ssp_transform(data, AEPz, CLz):
    cmap = mpl.cm.viridis
    cmap1 = mpl.cm.viridis_r
    fig = plt.figure(1, figsize=(24, 6))
    plt.clf()
    ax1 = plt.subplot2grid((1, 2), (0, 0), xlabel='Annual Exceedance Probability, [z variate]',
                           ylabel='Discharge, [log(cfs)]',
                           title='Discharge vs. Annual Exceedance Probability (Each Line is a Confidence Limit)')
    for i in np.arange(len(CLz)):
        ax1.plot(AEPz, data.iloc[:, i], linestyle="-", marker='.', color=cmap(i / len(CLz)))
    # ax1.set_xlim([-2.5,5.5])
    # ax1.set_ylim([2.5,6.5])
    ax1 = plt.subplot2grid((1, 2), (0, 1), xlabel='Confidence Limits, [z variate]', ylabel='Discharge, [log(cfs)]',
                           title='Discharge vs. Confidence Limits (Each Line is an Annual Exceedance Probability)')
    for i in np.arange(len(AEPz)):
        ax1.plot(CLz, data.iloc[i], linestyle="-", marker='.', color=cmap1(i / len(AEPz)))
    # ax1.set_xlim([-2.5,2.5])
    # ax1.set_ylim([2.5,6.5])


def plot_ssp_interp(df2, CLz):
    cmap = mpl.cm.viridis
    fig = plt.figure(2, figsize=(12, 6))
    plt.clf()
    ax1 = plt.subplot2grid((1, 1), (0, 0), xlabel='Confidence Limits, [z variate]',
                           ylabel='Annual Exceedance Probability, [z variate]',
                           title='Annual Exceedance Probability vs. Confidence Limits (Each Line is a Discharge)')
    for i in np.arange(len(df2.iloc[:])):
        ax1.plot(CLz, df2.iloc[i], linestyle="-", marker='.', color=cmap(i / len(df2.iloc[:])))
    # ax1.set_xlim([-2.5,2.5])
    # ax1.set_ylim([-5,20])


def plot_ssp_interptrans(df3):
    cmap = mpl.cm.viridis
    fig = plt.figure(3, figsize=(12, 6))
    plt.clf()
    ax1 = plt.subplot2grid((1, 1), (0, 0), xlabel='Confidence Limits', ylabel='Annual Exceedance Probability',
                           title='Annual Exceedance Probability vs. Confidence Limits (Each Line is a Discharge)')
    for i in np.arange(len(df3.index)):
        ax1.plot(df3.columns.values, df3.iloc[i], linestyle="-", marker='.', color=cmap(i / len(df3.index)))
    # ax1.set_xlim([0,1])
    # ax1.set_ylim([0,1.2])


def plot_ssp_meanmed(AEPz, df1, AEPmz, Q):
    fig = plt.figure(4, figsize=(12, 6))
    plt.clf()
    ax1 = plt.subplot2grid((1, 1), (0, 0), xlabel='Annual Exceedance Probability, [z variate]',
                           ylabel='Discharge, [log(cfs)]',
                           title='Discharge vs. Annual Exceedance Probability, with Mean and Median Curves')
    ax1.plot(AEPz, df1.iloc[:]['0.5'], linestyle="-", marker='.', label='Median', color='black')
    ax1.plot(AEPmz, linestyle="-", marker='.', label='Mean', color='red')
    # ax1.set_xlim([-2.5,7.5])
    # ax1.set_ylim([2.5,6.5])
    ax1.legend(loc='lower right', frameon=False)


def plot_ssp_meanmedffc(table, gage_ID, iplot=False):
    fig, ax = plt.subplots(1, 2, figsize=(24, 6))
    ax[0].plot([i * 100 for i in table.index], table['Q_Median_cfs'], linestyle="-", marker='.', label='Median',
               color='black')
    ax[0].plot([i * 100 for i in table.index], table['Q_Mean_cfs'], linestyle="-", marker='.', label='Mean',
               color='red')
    ax[0].set_xscale('log')
    ax[0].set_yscale('log')
    ax[0].set_xlabel('Annual Exceedance Probability, [%]')
    ax[0].set_ylabel('Discharge, [cfs]')
    ax[0].set_title('Discharge vs. Annual Exceedance Probability (Station ID:%s)' % gage_ID)
    ax[0].grid(True, which="both")
    ax[0].legend(loc='lower left', frameon=True)
    ax[0].set_xticklabels(['{:}'.format(x) for x in ax[0].get_xticks()])

    ax[1].plot([1.0 / i for i in table.index], table['Q_Median_cfs'], linestyle="-", marker='.', label='Median',
               color='black')
    ax[1].plot([1.0 / i for i in table.index], table['Q_Mean_cfs'], linestyle="-", marker='.', label='Mean',
               color='red')
    ax[1].set_xscale('log')
    ax[1].set_yscale('log')
    ax[1].set_xlabel('Recurrence Interval')
    ax[1].set_ylabel('Discharge, [cfs]')
    ax[1].set_title('Discharge vs. Recurrence Interval (Station ID:%s)' % gage_ID)
    ax[1].grid(True, which="both")
    ax[1].legend(loc='lower right', frameon=True)
    if iplot:
        plt.close(fig)
    return fig


def plot_ssp_meanmedffc_events(table, df3, gage_ID):
    fig, ax = plt.subplots(1, 2, figsize=(24, 6))
    ax[0].plot([i * 100 for i in table.index], table['Q_Median_cfs'], linestyle="-", marker='', label='Median',
               markersize=12, color='black')
    ax[0].plot([i * 100 for i in table.index], table['Q_Mean_cfs'], linestyle="-", marker='', markersize=12,
               label='Mean', color='red')
    for i in range(0, len(df3.columns) - 1):
        ax[0].plot(df3.AEP, df3.iloc[:, [i]], linestyle="", marker='.', label='', color='blue')
    ax[0].set_xscale('log')
    ax[0].set_yscale('log')
    ax[0].set_xlabel('Annual Exceedance Probability, [%]')
    ax[0].set_ylabel('Discharge, [cfs]')
    ax[0].set_title('Discharge vs. Annual Exceedance Probability (Station ID:%s)' % gage_ID)
    ax[0].grid(True, which="both")
    ax[0].legend(loc='lower left', frameon=True)
    ax[0].set_xticklabels(['{:}'.format(x) for x in ax[0].get_xticks()])

    ax[1].plot([1.0 / i for i in table.index], table['Q_Median_cfs'], linestyle="-", marker='', label='Median',
               color='black')
    ax[1].plot([1.0 / i for i in table.index], table['Q_Mean_cfs'], linestyle="-", marker='', label='Mean', color='red')
    for i in range(0, len(df3.columns) - 1):
        ax[1].plot(df3.index, df3.iloc[:, [i]], linestyle="", marker='.', label='', color='blue')
    ax[1].set_xscale('log')
    ax[1].set_yscale('log')
    ax[1].set_xlabel('Recurrence Interval')
    ax[1].set_ylabel('Discharge, [cfs]')
    ax[1].set_title('Discharge vs. Recurrence Interval (Station ID:%s)' % gage_ID)
    ax[1].grid(True, which="both")
    ax[1].legend(loc='lower right', frameon=True)
    return fig


def plot_AEP_Q(df, interp):
    fig = plt.figure(1, figsize=(12, 6))
    plt.clf()
    ax1 = plt.subplot2grid((1, 1), (0, 0), xlabel='Annual Exceedance Probability, [z variate]',
                           ylabel='Discharge, [log(cfs)]', title='Discharge vs. Annual Exceedance Probability')
    ax1.plot(interp.AEPz, interp.Qlog_int, linestyle="-", marker='.', label='Interpolated', color='blue')
    ax1.plot(df.AEPz, df.Qlog, linestyle="", marker='.', label='StreamStats', color='purple', markersize=10)
    # ax1.set_xlim([-2.5,7.5])
    # ax1.set_ylim([2.5,6.5])
    ax1.legend(loc='lower right', frameon=False)


def plot_meanffc(interp, gage_ID, iplot=False):
    fig, ax = plt.subplots(1, 2, figsize=(24, 6))
    ax[0].plot([i * 100 for i in interp.index], interp.Q_int, linestyle="-", marker='.', label='Median', color='blue')
    #    ax[0].plot([i*100 for i in interp.index],Q3, linestyle="-",marker='.', label='Mean', color='red')
    ax[0].set_xscale('log')
    ax[0].set_yscale('log')
    ax[0].set_xlabel('Annual Exceedance Probability, [%]')
    ax[0].set_ylabel('Discharge, [cfs]')
    ax[0].set_title('Discharge vs. Annual Exceedance Probability (Station ID:%s)' % gage_ID)
    ax[0].grid(True, which="both")
    #    ax[0].legend(loc='lower left',frameon=True)
    ax[0].set_xticklabels(['{:}'.format(x) for x in ax[0].get_xticks()])

    ax[1].plot([1.0 / i for i in interp.index], interp.Q_int, linestyle="-", marker='.', label='Median', color='blue')
    #    ax[1].plot([1.0/i for i in interp.index],Q3, linestyle="-",marker='.', label='Mean', color='red')
    ax[1].set_xscale('log')
    ax[1].set_yscale('log')
    ax[1].set_xlabel('Recurrence Interval')
    ax[1].set_ylabel('Discharge, [cfs]')
    ax[1].set_title('Discharge vs. Recurrence Interval (Station ID:%s)' % gage_ID)
    ax[1].grid(True, which="both")
    #    ax[1].legend(loc='lower right',frameon=True)
    #    fig.savefig(os.path.join(path,'%s.png' %gage_ID))
    if not iplot:
        plt.ioff()
    return


def plot_mean_gageungage(table, interp, gage_ID, iplot=False):
    fig, ax = plt.subplots(1, 2, figsize=(24, 6))
    ax[0].plot([i * 100 for i in table.index], table['Q_Median_cfs'], linestyle="-", marker='.', label='Median (Gaged)',
               color='black')
    ax[0].plot([i * 100 for i in table.index], table['Q_Mean_cfs'], linestyle="-", marker='.', label='Mean (Gaged)',
               color='red')
    ax[0].plot([i * 100 for i in interp.index], interp.Q_int, linestyle="-", marker='.', label='Median (Ungaged)',
               color='blue')
    ax[0].set_xscale('log')
    ax[0].set_yscale('log')
    ax[0].set_xlabel('Annual Exceedance Probability, [%]')
    ax[0].set_ylabel('Discharge, [cfs]')
    ax[0].set_title('Discharge vs. Annual Exceedance Probability (Station ID:%s)' % gage_ID)
    ax[0].grid(True, which="both")
    ax[0].legend(loc='lower left', frameon=True)
    ax[0].set_xticklabels(['{:}'.format(x) for x in ax[0].get_xticks()])

    ax[1].plot([1.0 / i for i in table.index], table['Q_Median_cfs'], linestyle="-", marker='.', label='Median (Gaged)',
               color='black')
    ax[1].plot([1.0 / i for i in table.index], table['Q_Mean_cfs'], linestyle="-", marker='.', label='Mean (Gaged)',
               color='red')
    ax[1].plot([1.0 / i for i in interp.index], interp.Q_int, linestyle="-", marker='.', label='Median (Ungaged)',
               color='blue')
    ax[1].set_xscale('log')
    ax[1].set_yscale('log')
    ax[1].set_xlabel('Recurrence Interval')
    ax[1].set_ylabel('Discharge, [cfs]')
    ax[1].set_title('Discharge vs. Recurrence Interval (Station ID:%s)' % gage_ID)
    ax[1].grid(True, which="both")
    ax[1].legend(loc='lower right', frameon=True)
    if not iplot:
        plt.ioff()
    return fig


def sim_Events(runs, sorted_runs):
    fig, ax = plt.subplots(1, 2, figsize=(20, 6))
    fig.suptitle('Augusta, GA')
    runIDs, flows = runs.index, runs['Final Peak Flow']
    SrunIDs, Sflows = sorted_runs.index, sorted_runs['Final Peak Flow']
    ax[0].scatter(x=runIDs, y=flows, label='Peak Flow')
    ax[0].set_title('Simulated Events\n(as Sampled)')
    ax[0].legend()
    ax[0].grid()
    ax[0].set_xlabel('Event ID')
    ax[0].set_ylabel('Flow cfs')
    ax[1].scatter(x=SrunIDs, y=Sflows, label='Peak Flow')
    ax[1].set_title('Simulated Events\n(sorted by Magnitude)')
    ax[1].legend()
    ax[1].grid()
    ax[1].set_xlabel('Sorted Events')
    ax[1].set_ylabel('Flow cfs')
    return


def plotly_ssp_meanmedffc(table, gage_ID):
    x = table['AEP'] * 100
    y = table['Q_Mean_cfs']
    trace = go.Scatter(x=x, y=y,
                       name='Rp', mode='markers',
                       text=table.Return_Period_Year.apply(lambda x: "{0:,.0f}".format(x)) + '-year',
                       hoverinfo='text+name')
    trace2 = go.Scatter(x=x, y=y,
                        name='Q', mode='lines', text=table.Q_Mean_cfs.apply(lambda x: "{0:,.2f}".format(x)),
                        hoverinfo='text+name',
                        line=dict(
                            color=('rgb(204, 0, 153)')))
    trace3 = go.Scatter(x=x, y=y,
                        name='Event', mode='markers', text=table.index, hoverinfo='text+name')
    data = [trace, trace2, trace3]
    layout = go.Layout(dict(title='Mean Discharge vs. Annual Exceedance Probability (Station ID:%s)' % gage_ID),
                       xaxis=dict(title='Annual Exceedance Probability, [%]', type='log', autorange=True,
                                  tickmode='linear'),
                       yaxis=dict(title='Discharge, [cfs]', type='log', autorange=True), legend=dict(orientation="h"),
                       font=dict(color='rgb(0,0,0)'), paper_bgcolor='rgb(255,255,255)',
                       plot_bgcolor='rgb(255,255,255)',
                       showlegend=False)
    fig = go.Figure(data=data, layout=layout)
    interactive = iplot(fig)

# ---------------------------------------------------------------------------#
